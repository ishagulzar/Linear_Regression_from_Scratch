{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4dcb7e4",
   "metadata": {},
   "source": [
    "### Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8a380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24ddd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [[-0.05626683]\n",
      " [-1.79343559]\n",
      " [-0.41675785]\n",
      " [-2.1361961 ]\n",
      " [ 1.64027081]]\n",
      "y:  [ -2.65249981 -38.12597924  -9.37063979 -42.8233715   34.30746371]\n",
      "size:  5\n"
     ]
    }
   ],
   "source": [
    "# Assigning the value 5 to the variable n, which represents the number of data samples we want to generate.\n",
    "n = 5\n",
    "\n",
    "# Generating synthetic regression data using scikit-learn's make_regression function.\n",
    "X, y = make_regression(n_samples=n, n_features=1, noise=1, random_state=2)\n",
    "\n",
    "print(\"X: \", X)\n",
    "print(\"y: \", y)\n",
    "print(\"size: \", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe65a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7429c9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  1\n"
     ]
    }
   ],
   "source": [
    "# Variable definition\n",
    "b0 = 1\n",
    "b1 = 1\n",
    "\n",
    "print(b0, \", \", b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9798b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function named yhat that calculates the predicted values (y-hat) for a simple linear regression model.\n",
    "\n",
    "def yhat(b0, b1, X):\n",
    "    return b0 + (b1 * X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff25afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94373317 -0.79343559  0.58324215 -1.1361961   2.64027081]\n",
      "[ -3.59623298 -37.33254365  -9.95388194 -41.6871754   31.66719291]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the predicted values (yhat_array) using the yhat function and then computing the error between the true values (y) and the predicted values.\n",
    "\n",
    "yhat_array = yhat(b0, b1, X)\n",
    "\n",
    "error = y - yhat_array\n",
    "print(yhat_array)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3273466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741.8263378367806"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function named mse (Mean Squared Error) and then calculating the Mean Squared Error between the true values y and the predicted values yhat_array\n",
    "\n",
    "def mse(y, yhat):\n",
    "    error = y - yhat\n",
    "    cost = (1/n) * (np.sum(error) ** 2)\n",
    "    return cost\n",
    "\n",
    "mse(y, yhat_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca83a726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.36105642761464"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function named gradient_wrt_b0 and then calculating the gradient of the Mean Squared Error (MSE) with respect to the parameter b0 of a linear regression model\n",
    "\n",
    "def gradient_wrt_b0(error, n):\n",
    "    return (-2 / n) * np.sum(error)\n",
    "gradient_wrt_b0(error, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d16dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-84.91958909873587"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a function named gradient_wrt_b1 and then calculating the gradient of the Mean Squared Error (MSE) with respect to the parameter b1 of a linear regression model.\n",
    "\n",
    "def gradient_wrt_b1(error, X, n):\n",
    "    return (-2 / n) * np.sum(error * X[:, 0])\n",
    "\n",
    "gradient_wrt_b1(error, X, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d35eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0 , b0:  0.7563894357238536  b1:  1.8491958909873587  cost:  657.5471794722478\n",
      "iteration:  100 , b0:  -1.4792604723081058  b1:  19.975991086944937  cost:  3.0617891874517174\n",
      "iteration:  200 , b0:  -0.6050592270787869  b1:  20.485476748767947  cost:  0.1801408566456587\n",
      "iteration:  300 , b0:  -0.4108423119705056  b1:  20.565910408960068  cost:  0.008012694647075663\n",
      "iteration:  400 , b0:  -0.37000525986221006  b1:  20.582574238067913  cost:  0.00035288871516660474\n",
      "iteration:  500 , b0:  -0.3614361882333373  b1:  20.5860689114131  cost:  1.5535758558068982e-05\n",
      "iteration:  600 , b0:  -0.3596382316621167  b1:  20.586802145299735  cost:  6.839444940456353e-07\n",
      "iteration:  700 , b0:  -0.3592609868109803  b1:  20.58695599132778  cost:  3.01098790501371e-08\n",
      "iteration:  800 , b0:  -0.3591818338082528  b1:  20.586988271091577  cost:  1.3255531773569308e-09\n",
      "iteration:  900 , b0:  -0.35916522603270695  b1:  20.586995043987773  cost:  5.835597086016379e-11\n",
      "iteration:  999 , b0:  -0.3591617559735351  b1:  20.586996459129164  cost:  2.6505534394044503e-12\n"
     ]
    }
   ],
   "source": [
    "# Implementing a simple gradient descent optimization algorithm to update the parameters b0 and b1 of a linear regression model iteratively in order to minimize the Mean Squared Error (MSE). \n",
    "\n",
    "iterations = 1000\n",
    "for i in range(1000):\n",
    "    yhat_array = yhat(b0, b1, X)\n",
    "    error = y - yhat_array\n",
    "    b0 = b0 - alpha * gradient_wrt_b0(error, n)\n",
    "    b1 = b1 - alpha * gradient_wrt_b1(error, X, n)\n",
    "    yhat_array = yhat(b0, b1, X)\n",
    "    cost = mse(y, yhat_array)\n",
    "    if(i % 100 == 0) | (i == iterations - 1):\n",
    "        print(\"iteration: \", i, \", b0: \", b0, \" b1: \", b1, \" cost: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22879bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept:  -0.3591608161325244\n",
      "coefficients:  [20.58699684]\n"
     ]
    }
   ],
   "source": [
    "# Fitting a linear regression model to the data and then printing the intercept and coefficients of the fitted model.\n",
    "\n",
    "X_sklearn = X\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_sklearn, y)\n",
    "print('intercept: ', model.intercept_)\n",
    "print('coefficients: ', model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a47fcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05626683],\n",
       "       [-1.79343559],\n",
       "       [-0.41675785],\n",
       "       [-2.1361961 ],\n",
       "       [ 1.64027081]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping the NumPy array\n",
    "    \n",
    "np.array(X.reshape(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b112a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\anaconda\\lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2193.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 26 Sep 2023</td> <th>  Prob (F-statistic):</th> <td>2.14e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>00:43:14</td>     <th>  Log-Likelihood:    </th> <td> -7.2344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     5</td>      <th>  AIC:               </th> <td>   18.47</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     3</td>      <th>  BIC:               </th> <td>   17.69</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -0.3592</td> <td>    0.641</td> <td>   -0.560</td> <td> 0.615</td> <td>   -2.401</td> <td>    1.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   20.5870</td> <td>    0.440</td> <td>   46.833</td> <td> 0.000</td> <td>   19.188</td> <td>   21.986</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   0.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.376</td> <th>  Prob(JB):          </th> <td>   0.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 1.450</td> <th>  Cond. No.          </th> <td>    1.74</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.999   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.998   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     2193.   \\\\\n",
       "\\textbf{Date:}             & Tue, 26 Sep 2023 & \\textbf{  Prob (F-statistic):} &  2.14e-05   \\\\\n",
       "\\textbf{Time:}             &     00:43:14     & \\textbf{  Log-Likelihood:    } &   -7.2344   \\\\\n",
       "\\textbf{No. Observations:} &           5      & \\textbf{  AIC:               } &     18.47   \\\\\n",
       "\\textbf{Df Residuals:}     &           3      & \\textbf{  BIC:               } &     17.69   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      -0.3592  &        0.641     &    -0.560  &         0.615        &       -2.401    &        1.682     \\\\\n",
       "\\textbf{x1}    &      20.5870  &        0.440     &    46.833  &         0.000        &       19.188    &       21.986     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &    nan & \\textbf{  Durbin-Watson:     } &    0.836  \\\\\n",
       "\\textbf{Prob(Omnibus):} &    nan & \\textbf{  Jarque-Bera (JB):  } &    0.619  \\\\\n",
       "\\textbf{Skew:}          &  0.376 & \\textbf{  Prob(JB):          } &    0.734  \\\\\n",
       "\\textbf{Kurtosis:}      &  1.450 & \\textbf{  Cond. No.          } &     1.74  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.999\n",
       "Model:                            OLS   Adj. R-squared:                  0.998\n",
       "Method:                 Least Squares   F-statistic:                     2193.\n",
       "Date:                Tue, 26 Sep 2023   Prob (F-statistic):           2.14e-05\n",
       "Time:                        00:43:14   Log-Likelihood:                -7.2344\n",
       "No. Observations:                   5   AIC:                             18.47\n",
       "Df Residuals:                       3   BIC:                             17.69\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.3592      0.641     -0.560      0.615      -2.401       1.682\n",
       "x1            20.5870      0.440     46.833      0.000      19.188      21.986\n",
       "==============================================================================\n",
       "Omnibus:                          nan   Durbin-Watson:                   0.836\n",
       "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.619\n",
       "Skew:                           0.376   Prob(JB):                        0.734\n",
       "Kurtosis:                       1.450   Cond. No.                         1.74\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing Ordinary Least Squares (OLS) linear regression using the Statsmodels library in Python and then printing a summary of the regression results.\n",
    "\n",
    "X_ols = sm.add_constant(X)\n",
    "res = stats_model_regression = sm.OLS(y, X_ols).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0ef56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
